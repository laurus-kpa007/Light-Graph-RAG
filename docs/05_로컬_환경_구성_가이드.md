# 로컬 환경 구성 가이드

## 목차
1. [시스템 요구사항](#1-시스템-요구사항)
2. [Windows 환경 구성](#2-windows-환경-구성)
3. [macOS 환경 구성](#3-macos-환경-구성)
4. [실행 방법](#4-실행-방법)
5. [문제 해결](#5-문제-해결)

---

## 1. 시스템 요구사항

### 최소 요구사항
- **OS**: Windows 10/11 또는 macOS 10.15+
- **Python**: 3.10 이상
- **RAM**: 8GB 이상
- **저장공간**: 10GB 이상 여유 공간
- **인터넷**: 초기 설치 시 필요

### 권장 요구사항
- **RAM**: 16GB 이상
- **GPU**: NVIDIA GPU (VRAM 8GB 이상) - 선택사항
- **SSD**: 더 빠른 인덱싱 성능

---

## 2. Windows 환경 구성

### 2.1 Python 설치

#### 방법 1: 공식 웹사이트에서 설치
1. [Python 공식 사이트](https://www.python.org/downloads/) 접속
2. Python 3.10 이상 버전 다운로드
3. 설치 시 **"Add Python to PATH"** 체크 필수
4. 설치 완료 후 PowerShell 또는 CMD에서 확인:
   ```powershell
   python --version
   ```

#### 방법 2: Microsoft Store에서 설치
1. Microsoft Store 실행
2. "Python 3.12" 검색 및 설치
3. 설치 완료 후 확인:
   ```powershell
   python --version
   ```

### 2.2 Git 설치

1. [Git for Windows](https://git-scm.com/download/win) 다운로드
2. 설치 (기본 옵션 사용)
3. Git Bash 또는 PowerShell에서 확인:
   ```powershell
   git --version
   ```

### 2.3 Ollama 설치

#### Step 1: Ollama 다운로드 및 설치
1. [Ollama 공식 사이트](https://ollama.com/download/windows) 접속
2. Windows용 설치 파일 다운로드
3. 설치 파일 실행 (OllamaSetup.exe)
4. 설치 완료 후 자동으로 백그라운드에서 실행됨

#### Step 2: 설치 확인
PowerShell 또는 CMD에서:
```powershell
ollama --version
```

#### Step 3: 필요한 모델 다운로드
```powershell
# Gemma2 모델 다운로드 (약 2GB)
ollama pull gemma2:latest

# BGE-M3 임베딩 모델 다운로드 (약 500MB)
ollama pull bge-m3:latest
```

**모델 다운로드 진행 중 표시 예시:**
```
pulling manifest
pulling 8934d96d3f08... 100% ████████████████ 1.7 GB
pulling 8c17c2ebb0ea... 100% ████████████████ 7.0 KB
pulling 7c23fb36d801... 100% ████████████████ 4.8 KB
pulling 2e0493f67d0c... 100% ████████████████   59 B
pulling fa304d675061... 100% ████████████████   91 B
pulling 42347cd80dc8... 100% ████████████████  485 B
verifying sha256 digest
writing manifest
removing any unused layers
success
```

#### Step 4: Ollama 서비스 확인
```powershell
# Ollama가 실행 중인지 확인
ollama list
```

**출력 예시:**
```
NAME                ID              SIZE    MODIFIED
gemma2:latest       8934d96d3f08    1.7 GB  2 minutes ago
bge-m3:latest       8c17c2ebb0ea    500 MB  1 minute ago
```

### 2.4 프로젝트 클론 및 설정

#### Step 1: 프로젝트 클론
```powershell
# 원하는 디렉토리로 이동
cd C:\Users\YourName\Documents

# 프로젝트 클론
git clone https://github.com/laurus-kpa007/Light-Graph-RAG.git

# 프로젝트 디렉토리로 이동
cd Light-Graph-RAG
```

#### Step 2: 가상환경 생성
```powershell
# 가상환경 생성
python -m venv venv

# 가상환경 활성화
.\venv\Scripts\activate

# 활성화 확인 (프롬프트에 (venv) 표시됨)
```

**가상환경 활성화가 안 되는 경우:**
```powershell
# PowerShell 실행 정책 변경 (관리자 권한 필요)
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# 다시 활성화 시도
.\venv\Scripts\activate
```

#### Step 3: 의존성 설치
```powershell
# pip 업그레이드
python -m pip install --upgrade pip

# 의존성 설치
pip install -r requirements.txt
```

**설치 진행 시간:** 약 3-5분 소요

#### Step 4: 환경 변수 설정 (선택사항)
```powershell
# .env 파일 생성
copy .env.example .env

# 메모장으로 .env 파일 편집
notepad .env
```

**.env 파일 내용:**
```env
OLLAMA_HOST=http://localhost:11434
LLM_MODEL=gemma2:latest
EMBEDDING_MODEL=bge-m3:latest
PROJECT_ROOT=.
INDEX_DIR=./data/index
LOG_LEVEL=INFO
```

### 2.5 디렉토리 구조 확인

```powershell
# 프로젝트 구조 확인
tree /F /A
```

**예상 출력:**
```
Light-Graph-RAG
├── data/
│   ├── raw/          (여기에 .docx, .md 파일 업로드)
│   ├── processed/
│   └── index/
├── docs/
├── src/
├── scripts/
├── requirements.txt
└── README.md
```

---

## 3. macOS 환경 구성

### 3.1 Homebrew 설치 (필요시)

```bash
# Homebrew 설치 (이미 설치되어 있으면 건너뛰기)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

### 3.2 Python 설치

```bash
# Homebrew로 Python 설치
brew install python@3.11

# 설치 확인
python3 --version
```

### 3.3 Git 설치

```bash
# Git 설치 (보통 macOS에 기본 설치되어 있음)
brew install git

# 확인
git --version
```

### 3.4 Ollama 설치

#### Step 1: Ollama 다운로드 및 설치
1. [Ollama macOS 다운로드](https://ollama.com/download/mac) 접속
2. Ollama.app 다운로드
3. Ollama.app을 Applications 폴더로 이동
4. Ollama 실행 (메뉴바에 아이콘 표시됨)

**또는 Homebrew로 설치:**
```bash
brew install ollama
```

#### Step 2: Ollama 서비스 시작
```bash
# Ollama 서비스 시작
ollama serve &
```

#### Step 3: 모델 다운로드
```bash
# Gemma2 모델 다운로드
ollama pull gemma2:latest

# BGE-M3 임베딩 모델 다운로드
ollama pull bge-m3:latest

# 다운로드 확인
ollama list
```

### 3.5 프로젝트 클론 및 설정

#### Step 1: 프로젝트 클론
```bash
# 홈 디렉토리로 이동
cd ~

# 프로젝트 클론
git clone https://github.com/laurus-kpa007/Light-Graph-RAG.git

# 프로젝트 디렉토리로 이동
cd Light-Graph-RAG
```

#### Step 2: 가상환경 생성
```bash
# 가상환경 생성
python3 -m venv venv

# 가상환경 활성화
source venv/bin/activate

# 활성화 확인 (프롬프트에 (venv) 표시됨)
```

#### Step 3: 의존성 설치
```bash
# pip 업그레이드
pip install --upgrade pip

# 의존성 설치
pip install -r requirements.txt
```

#### Step 4: 환경 변수 설정 (선택사항)
```bash
# .env 파일 생성
cp .env.example .env

# 편집
nano .env
# 또는
vim .env
```

---

## 4. 실행 방법

### 4.1 WebUI 실행

#### Windows (PowerShell)
```powershell
# 가상환경 활성화 (이미 활성화되어 있으면 건너뛰기)
.\venv\Scripts\activate

# WebUI 실행
python scripts/run_webui.py
```

#### macOS/Linux
```bash
# 가상환경 활성화
source venv/bin/activate

# WebUI 실행
python scripts/run_webui.py
```

### 4.2 접속

WebUI가 실행되면 다음과 같은 메시지가 표시됩니다:

```
============================================================
Light GraphRAG - 사내 규정 질의응답 시스템
============================================================

설정 정보:
  - LLM 모델: gemma2:latest
  - 임베딩 모델: bge-m3:latest
  - Ollama 주소: http://localhost:11434
  - 인덱스 디렉토리: D:\Python\Light-Graph-RAG\data\index

WebUI 시작 중...
Running on local URL:  http://127.0.0.1:7860

To create a public link, set `share=True` in `launch()`.
```

**브라우저에서 접속:**
```
http://localhost:7860
```

### 4.3 문서 인덱싱

1. **문서 관리** 탭 클릭
2. **.docx** 또는 **.md** 파일 업로드
3. **인덱싱 시작** 버튼 클릭
4. 진행 상황 확인

### 4.4 질의응답

1. **질의응답** 탭 클릭
2. 질문 입력 (예: "연차 사용 규정은?")
3. 검색 모드 선택 (기본: hybrid)
4. **검색** 버튼 클릭
5. 답변 확인

---

## 5. 문제 해결

### 5.1 Ollama 연결 실패

**증상:**
```
검색 중 오류 발생: Connection error
```

**해결 방법:**

#### Windows
```powershell
# 1. Ollama가 실행 중인지 확인
ollama list

# 2. Ollama 재시작
# 작업 관리자에서 Ollama 프로세스 종료 후 다시 실행

# 3. 포트 확인
netstat -ano | findstr :11434
```

#### macOS
```bash
# 1. Ollama 상태 확인
ps aux | grep ollama

# 2. Ollama 재시작
killall ollama
ollama serve &

# 3. 포트 확인
lsof -i :11434
```

### 5.2 모듈 설치 오류

**증상:**
```
ERROR: Could not find a version that satisfies the requirement lightrag-hku
```

**해결 방법:**
```bash
# pip 업그레이드
pip install --upgrade pip

# 개별 설치 시도
pip install lightrag-hku
pip install markitdown
pip install gradio

# 캐시 삭제 후 재설치
pip cache purge
pip install -r requirements.txt
```

### 5.3 가상환경 활성화 오류 (Windows)

**증상:**
```
.\venv\Scripts\activate : 이 시스템에서 스크립트를 실행할 수 없으므로...
```

**해결 방법:**
```powershell
# PowerShell을 관리자 권한으로 실행
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# 다시 활성화
.\venv\Scripts\activate
```

### 5.4 메모리 부족 오류

**증상:**
```
RuntimeError: CUDA out of memory
```

**해결 방법:**

1. **배치 크기 줄이기**
   - `src/webui/gradio_app.py`에서 `batch_size=10`을 `batch_size=5`로 변경

2. **청크 크기 조정**
   - `src/utils/config.py`에서 `chunk_size` 줄이기

3. **CPU 모드 사용**
   - GPU 대신 CPU 사용 (속도는 느리지만 안정적)

### 5.5 한글 인코딩 오류

**증상:**
```
UnicodeDecodeError: 'utf-8' codec can't decode byte...
```

**해결 방법:**

Windows:
```powershell
# 시스템 로케일 설정 확인
chcp 65001  # UTF-8로 변경

# 환경 변수 설정
$env:PYTHONIOENCODING="utf-8"
```

macOS:
```bash
# ~/.zshrc 또는 ~/.bash_profile에 추가
export LANG=ko_KR.UTF-8
export LC_ALL=ko_KR.UTF-8
```

### 5.6 포트 충돌

**증상:**
```
OSError: [Errno 98] Address already in use
```

**해결 방법:**

Windows:
```powershell
# 7860 포트 사용 중인 프로세스 확인
netstat -ano | findstr :7860

# 프로세스 종료 (PID 확인 후)
taskkill /PID <PID> /F

# 또는 다른 포트 사용
# run_webui.py에서 server_port=7861로 변경
```

macOS:
```bash
# 포트 사용 중인 프로세스 확인
lsof -i :7860

# 프로세스 종료
kill -9 <PID>
```

---

## 6. 추가 설정

### 6.1 GPU 가속 활성화 (NVIDIA GPU 사용 시)

#### Windows
```powershell
# CUDA Toolkit 설치 확인
nvidia-smi

# PyTorch with CUDA 설치 (선택사항)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### macOS (Apple Silicon)
```bash
# Metal 가속은 자동으로 활성화됨
# 별도 설정 불필요
```

### 6.2 로그 레벨 조정

`.env` 파일 수정:
```env
LOG_LEVEL=DEBUG  # 더 자세한 로그
# 또는
LOG_LEVEL=WARNING  # 경고만 표시
```

### 6.3 데이터 백업

```bash
# 인덱스 데이터 백업
# Windows
xcopy /E /I data\index data\index_backup

# macOS/Linux
cp -r data/index data/index_backup
```

---

## 7. 테스트 실행

### 7.1 간단한 테스트

```bash
# Python 인터프리터 실행
python

# 테스트 코드
>>> from src.preprocessing import DocumentConverter
>>> converter = DocumentConverter()
>>> print("전처리 모듈 정상 작동")
>>> exit()
```

### 7.2 Ollama 연결 테스트

```bash
# Ollama 테스트
ollama run gemma2:latest "Hello"

# 정상 응답이 오면 OK
```

### 7.3 샘플 문서로 테스트

1. `data/raw/` 폴더에 테스트 문서 생성:

**test.md** 파일:
```markdown
# 테스트 규정

## 제1조 (목적)
이 문서는 테스트용입니다.

## 제2조 (내용)
시스템이 정상 작동하는지 확인합니다.
```

2. WebUI에서 업로드 및 인덱싱
3. "목적은 무엇인가요?" 질문 테스트

---

## 8. 성능 최적화 팁

### Windows
1. Windows Defender 예외 처리:
   - 프로젝트 폴더를 Windows Defender 검사 제외 목록에 추가

2. 전원 관리:
   - 고성능 모드로 변경

### macOS
1. Spotlight 색인 제외:
   ```bash
   # 프로젝트 폴더를 Spotlight 색인에서 제외
   ```

2. 백그라운드 앱 정리:
   - Activity Monitor에서 불필요한 앱 종료

---

## 9. 지원 및 문서

### 공식 문서
- [README.md](../README.md) - 프로젝트 개요
- [시스템 아키텍처](./01_시스템_아키텍처_설계.md)
- [API 명세](./04_API_인터페이스_명세.md)

### 외부 문서
- [Ollama 공식 문서](https://ollama.com/docs)
- [LightRAG GitHub](https://github.com/HKUDS/LightRAG)
- [Gradio 문서](https://www.gradio.app/docs)

### 이슈 보고
- GitHub Issues: https://github.com/laurus-kpa007/Light-Graph-RAG/issues

---

**작성일**: 2026-01-17
**버전**: 1.0
