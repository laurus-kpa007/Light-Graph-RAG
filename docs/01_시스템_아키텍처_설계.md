# Light GraphRAG 시스템 아키텍처 설계

## 1. 프로젝트 개요

### 1.1 목적
사내 규정 문서(.docx, .md)에 대한 질의응답이 가능한 로컬 LLM 기반 Light GraphRAG 시스템 구축

### 1.2 핵심 기술 스택
- **LLM**: Ollama + Gemma3
- **Embedding**: Ollama + bge-m3
- **RAG 프레임워크**: lightrag-hku
- **문서 변환**: markitdown
- **Web UI**: Gradio
- **언어**: Python 3.10+

---

## 2. 시스템 전체 아키텍처

```mermaid
graph TB
    subgraph "입력 계층"
        A1[.docx 파일들] --> B[문서 전처리 모듈]
        A2[.md 파일들] --> B
    end

    subgraph "전처리 계층"
        B --> C[문서 변환기<br/>.docx→MD / .md 로드]
        C --> D[마크다운 파서]
        D --> E[청크 분할기]
    end

    subgraph "LLM 서비스 계층"
        F[Ollama Server]
        F --> G[Gemma3 모델<br/>텍스트 생성]
        F --> H[BGE-M3 모델<br/>임베딩 생성]
    end

    subgraph "RAG 엔진 계층"
        E --> I[LightRAG 엔진]
        I --> J[엔티티 추출기<br/>한국어 최적화]
        I --> K[그래프 빌더]
        I --> L[벡터 인덱서]

        J -.-> G
        K -.-> G
        L -.-> H
    end

    subgraph "저장 계층"
        K --> M[(지식 그래프<br/>Neo4j-like Storage)]
        L --> N[(벡터 DB<br/>ChromaDB)]
        I --> O[(문서 메타데이터<br/>SQLite)]
    end

    subgraph "검색 계층"
        P[하이브리드 검색 엔진]
        P --> Q[그래프 검색]
        P --> R[벡터 검색]
        P --> S[검색 결과 융합]

        Q -.-> M
        R -.-> N
    end

    subgraph "인터페이스 계층"
        T[Gradio Web UI]
        T --> U[질문 입력]
        U --> P
        S --> V[답변 생성기]
        V -.-> G
        V --> W[답변 출력]
        W --> T
    end

    style I fill:#ff9999
    style J fill:#ffcc99
    style P fill:#99ccff
    style T fill:#99ff99
```

---

## 3. 핵심 컴포넌트 구성

### 3.1 시스템 계층 구조

```mermaid
graph LR
    subgraph "Layer 1: Presentation"
        UI[Web UI<br/>Gradio]
    end

    subgraph "Layer 2: Application"
        API[FastAPI/Flask<br/>REST API]
        SEARCH[검색 서비스]
        INDEX[인덱싱 서비스]
    end

    subgraph "Layer 3: Business Logic"
        LIGHTRAG[LightRAG Core]
        PREPROC[전처리 엔진]
        PROMPT[프롬프트 관리자]
    end

    subgraph "Layer 4: Data Access"
        GRAPH[그래프 저장소]
        VECTOR[벡터 저장소]
        META[메타데이터 DB]
    end

    subgraph "Layer 5: External Services"
        OLLAMA[Ollama Server]
    end

    UI --> API
    API --> SEARCH
    API --> INDEX
    SEARCH --> LIGHTRAG
    INDEX --> PREPROC
    PREPROC --> LIGHTRAG
    LIGHTRAG --> PROMPT
    LIGHTRAG --> GRAPH
    LIGHTRAG --> VECTOR
    LIGHTRAG --> META
    LIGHTRAG --> OLLAMA

    style LIGHTRAG fill:#ff9999
    style PROMPT fill:#ffcc99
```

---

## 4. 주요 모듈 구성

### 4.1 프로젝트 디렉토리 구조

```
Light-Graph-RAG/
├── docs/                           # 설계 문서
│   ├── 01_시스템_아키텍처_설계.md
│   ├── 02_모듈별_상세_설계.md
│   ├── 03_데이터_플로우_설계.md
│   └── 04_API_인터페이스_명세.md
│
├── src/                            # 소스 코드
│   ├── __init__.py
│   │
│   ├── preprocessing/              # 전처리 모듈
│   │   ├── __init__.py
│   │   ├── docx_converter.py      # .docx → markdown 변환
│   │   ├── markdown_parser.py     # 마크다운 파싱 및 구조화
│   │   └── chunker.py             # 청크 분할 로직
│   │
│   ├── rag/                        # RAG 엔진 모듈
│   │   ├── __init__.py
│   │   ├── lightrag_wrapper.py    # LightRAG 초기화 및 래핑
│   │   ├── korean_prompts.py      # 한국어 최적화 프롬프트
│   │   ├── indexer.py             # 문서 인덱싱 로직
│   │   └── searcher.py            # 하이브리드 검색 구현
│   │
│   ├── llm/                        # LLM 인터페이스 모듈
│   │   ├── __init__.py
│   │   ├── ollama_client.py       # Ollama 클라이언트
│   │   └── model_config.py        # 모델 설정 관리
│   │
│   ├── webui/                      # Web UI 모듈
│   │   ├── __init__.py
│   │   ├── gradio_app.py          # Gradio 인터페이스
│   │   └── ui_components.py       # UI 컴포넌트
│   │
│   └── utils/                      # 유틸리티 모듈
│       ├── __init__.py
│       ├── config.py              # 전역 설정
│       ├── logger.py              # 로깅 유틸
│       └── validators.py          # 검증 함수
│
├── data/                           # 데이터 디렉토리
│   ├── raw/                        # 원본 .docx 파일
│   ├── processed/                  # 전처리된 마크다운
│   └── index/                      # LightRAG 인덱스 저장소
│       ├── graph/                  # 지식 그래프
│       ├── vector/                 # 벡터 DB
│       └── metadata/               # 메타데이터
│
├── tests/                          # 테스트 코드
│   ├── test_preprocessing.py
│   ├── test_rag.py
│   └── test_search.py
│
├── configs/                        # 설정 파일
│   ├── llm_config.yaml            # LLM 설정
│   ├── rag_config.yaml            # RAG 설정
│   └── prompts.yaml               # 프롬프트 템플릿
│
├── scripts/                        # 실행 스크립트
│   ├── setup.sh                   # 환경 설정
│   ├── run_preprocessing.py       # 전처리 실행
│   ├── run_indexing.py            # 인덱싱 실행
│   └── run_webui.py               # WebUI 실행
│
├── requirements.txt                # Python 의존성
├── README.md                       # 프로젝트 설명
└── .env.example                    # 환경 변수 예제
```

---

## 5. 데이터 흐름도

### 5.1 인덱싱 프로세스

```mermaid
sequenceDiagram
    participant User
    participant Preprocessor
    participant MarkItDown
    participant Chunker
    participant LightRAG
    participant Ollama
    participant Storage

    User->>Preprocessor: .docx 파일 업로드
    Preprocessor->>MarkItDown: 문서 변환 요청
    MarkItDown-->>Preprocessor: Markdown 텍스트

    Preprocessor->>Chunker: 청크 분할 요청
    Note over Chunker: 헤더 기준 분할<br/>의미 단위 유지
    Chunker-->>Preprocessor: 청크 리스트

    Preprocessor->>LightRAG: 인덱싱 요청

    loop 각 청크마다
        LightRAG->>Ollama: 엔티티 추출 (Gemma3)
        Ollama-->>LightRAG: 엔티티 목록

        LightRAG->>Ollama: 임베딩 생성 (BGE-M3)
        Ollama-->>LightRAG: 벡터 임베딩

        LightRAG->>Storage: 그래프 노드/엣지 저장
        LightRAG->>Storage: 벡터 인덱스 저장
    end

    LightRAG-->>User: 인덱싱 완료
```

### 5.2 검색 및 답변 프로세스

```mermaid
sequenceDiagram
    participant User
    participant WebUI
    participant Searcher
    participant GraphSearch
    participant VectorSearch
    participant Ollama
    participant Generator

    User->>WebUI: 질문 입력
    WebUI->>Searcher: 하이브리드 검색 요청

    par 병렬 검색
        Searcher->>GraphSearch: 그래프 검색
        GraphSearch-->>Searcher: 관련 엔티티/관계

        Searcher->>VectorSearch: 벡터 유사도 검색
        VectorSearch->>Ollama: 질문 임베딩 생성
        Ollama-->>VectorSearch: 질문 벡터
        VectorSearch-->>Searcher: 유사 문서 청크
    end

    Searcher->>Searcher: 검색 결과 융합
    Note over Searcher: Re-ranking<br/>중복 제거

    Searcher->>Generator: 컨텍스트 + 질문 전달
    Generator->>Ollama: 답변 생성 (Gemma3)
    Note over Ollama: 한국어 프롬프트<br/>컨텍스트 기반 생성
    Ollama-->>Generator: 생성된 답변

    Generator-->>WebUI: 최종 답변
    WebUI-->>User: 답변 표시
```

---

## 6. 핵심 설계 결정사항

### 6.1 한국어 프롬프트 최적화

**문제**: LightRAG의 기본 프롬프트는 영어 중심으로 설계되어 한국어 엔티티 추출 성능이 낮음

**해결책**:
```python
# src/rag/korean_prompts.py에서 커스터마이징
KOREAN_ENTITY_EXTRACTION_PROMPT = """
당신은 한국어 문서에서 핵심 개념과 엔티티를 추출하는 전문가입니다.

다음 텍스트에서 주요 엔티티(사람, 조직, 개념, 규정명 등)를 추출하고,
엔티티 간의 관계를 파악하세요.

텍스트:
{text}

출력 형식:
- 엔티티: [엔티티명, 타입]
- 관계: [엔티티1, 관계명, 엔티티2]
"""
```

### 6.2 하이브리드 검색 전략

**구성**:
1. **그래프 검색**: 엔티티 및 관계 기반 컨텍스트 탐색
2. **벡터 검색**: 의미적 유사도 기반 문서 검색
3. **결과 융합**: RRF(Reciprocal Rank Fusion) 알고리즘 사용

**구현**:
```python
# search_mode="hybrid" 설정
result = rag.query(
    question="질문",
    search_mode="hybrid",  # naive, local, global, hybrid
    top_k=10
)
```

### 6.3 청크 분할 전략

**원칙**:
- 마크다운 헤더(#, ##, ###)를 기준으로 계층적 분할
- 표(table) 구조는 분할하지 않고 하나의 청크로 유지
- 최소 청크 크기: 100자
- 최대 청크 크기: 1000자
- 오버랩: 50자 (문맥 연속성 보장)

---

## 7. 기술적 제약사항 및 고려사항

### 7.1 성능 제약
- **로컬 LLM**: Ollama 실행 시 GPU 메모리 최소 8GB 권장
- **인덱싱 속도**: Gemma3 모델 특성상 대량 문서 처리 시간 소요
- **동시 사용자**: 단일 인스턴스 기준 3-5명 동시 접속 권장

### 7.2 데이터 보안
- 모든 데이터는 로컬에서 처리 (외부 API 호출 없음)
- 사내 규정 문서의 민감성을 고려한 접근 제어 필요

### 7.3 확장성 고려
- 문서 수 증가 시 인덱스 재구성 전략 필요
- 모듈화된 설계로 향후 다른 LLM 교체 가능

---

## 8. 배포 및 운영

### 8.1 배포 환경
```mermaid
graph TB
    subgraph "배포 서버"
        A[Docker Container]
        A --> B[Ollama Service<br/>Gemma3 + BGE-M3]
        A --> C[Python Application<br/>LightRAG + WebUI]
        A --> D[Data Volume<br/>/data/index]
    end

    E[사용자 브라우저] -->|HTTP| C
    C -->|API Call| B
    C -->|Read/Write| D

    style A fill:#e1f5ff
```

### 8.2 모니터링 포인트
- Ollama 서버 상태 및 응답 시간
- 인덱스 크기 및 검색 성능
- 메모리 사용량 (특히 벡터 DB)
- 사용자 질의 로그 및 답변 품질

---

## 9. 다음 단계

1. ✅ 시스템 아키텍처 설계 완료
2. ⏳ 모듈별 상세 설계 (다음 문서)
3. ⏳ 데이터 플로우 상세 설계
4. ⏳ API 인터페이스 명세
5. ⏳ 구현 및 테스트

---

**작성일**: 2026-01-17
**버전**: 1.0
**작성자**: Claude Code
